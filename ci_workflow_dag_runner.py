# -*- coding: utf-8 -*-
"""CI Workflow DAG Runner

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y1sEyvWd3UiDWmKNrdVAn4E3hqiXSSDK
"""

# CI Workflow DAG Runner â€” fixed runs-block parsing (Colab-friendly, verbose)

import os, re, glob, hashlib, shutil, subprocess, time
from pathlib import Path

def read_text(p: str) -> str:
    with open(p, "r", encoding="utf-8") as f:
        return f.read()

def parse_yamlish(path: str) -> dict:
    """
    Minimal YAML-ish parser for:
      jobs:
        - name: build
          runs: |
            echo "hi"
          needs:
            - test
          retries: 1
          inputs:
            - src/**
          artifacts:
            - out/**
    """
    lines = [ln.rstrip("\n") for ln in read_text(path).splitlines()]

    jobs = []
    cur = None
    key = None
    in_runs = False
    runs_indent = None

    def ensure_lists():
        cur.setdefault("needs", [])
        cur.setdefault("inputs", [])
        cur.setdefault("artifacts", [])

    def finish_job():
        nonlocal cur
        if not cur:
            return
        cur["runs"] = (cur.get("runs") or "").rstrip("\n")
        cur["retries"] = int(cur.get("retries", 1))
        ensure_lists()
        jobs.append(cur)
        cur = None

    i = 0
    while i < len(lines):
        ln = lines[i]
        i += 1

        if not ln.strip() or ln.lstrip().startswith("#"):
            continue

        m = re.match(r"^\s*-\s+name:\s*(.+)$", ln)
        if m:
            finish_job()
            cur = {"name": m.group(1).strip(), "runs": ""}
            key = None
            in_runs = False
            runs_indent = None
            continue

        if cur is None:
            continue

        if re.match(r"^\s*runs:\s*\|\s*$", ln):
            in_runs = True
            runs_indent = None
            key = None
            cur["runs"] = ""
            continue

        if in_runs:
            # If first content line, set block indent
            if ln.strip():
                this_indent = len(ln) - len(ln.lstrip(" "))
                if runs_indent is None:
                    runs_indent = this_indent
                # If indentation drops below runs block, end block and reprocess this line
                if this_indent < runs_indent:
                    in_runs = False
                    i -= 1  # step back to reprocess this line as a key
                    continue
                cur["runs"] += ln[runs_indent:] + "\n"
                continue
            else:
                cur["runs"] += "\n"
                continue

        # Section headers
        if re.match(r"^\s*needs:\s*$", ln):
            key = "needs"; ensure_lists(); continue
        if re.match(r"^\s*inputs:\s*$", ln):
            key = "inputs"; ensure_lists(); continue
        if re.match(r"^\s*artifacts:\s*$", ln):
            key = "artifacts"; ensure_lists(); continue

        # Scalar
        m = re.match(r"^\s*retries:\s*(\d+)\s*$", ln)
        if m:
            cur["retries"] = int(m.group(1)); key = None; continue

        # Items under the active section
        if re.match(r"^\s*-\s+.*", ln) and key in ("needs", "inputs", "artifacts"):
            ensure_lists()
            item = ln.split("-", 1)[1].strip()
            cur[key].append(item)
            continue

        # Tolerate unrecognized lines
        # print("WARN ignored:", ln)

    finish_job()
    return {"jobs": jobs}

def topo_sort(jobs):
    name_to_job = {j["name"]: j for j in jobs}
    indeg = {j["name"]: 0 for j in jobs}
    for j in jobs:
        for dep in j.get("needs", []):
            indeg[j["name"]] += 1
    q = [n for n, d in indeg.items() if d == 0]
    order = []
    while q:
        n = q.pop(0)
        order.append(name_to_job[n])
        for j in jobs:
            if n in j.get("needs", []):
                indeg[j["name"]] -= 1
                if indeg[j["name"]] == 0:
                    q.append(j["name"])
    if len(order) != len(jobs):
        raise RuntimeError("Cycle detected in job dependencies")
    return order

def hash_inputs(cmd: str, patterns: list[str]) -> str:
    h = hashlib.sha256()
    h.update(cmd.encode())
    files = set()
    for pat in patterns or []:
        files.update(glob.glob(pat, recursive=True))
    for p in sorted(files):
        if os.path.isfile(p):
            with open(p, "rb") as f:
                h.update(f.read())
    return h.hexdigest()[:16]

def collect_artifacts(jobname: str, patterns: list[str]):
    target = Path("artifacts") / jobname
    target.mkdir(parents=True, exist_ok=True)
    files = set()
    for pat in patterns or []:
        files.update(glob.glob(pat, recursive=True))
    for f in files:
        if os.path.isfile(f):
            shutil.copy2(f, target / os.path.basename(f))

def run_cmd_bash(cmd: str) -> subprocess.CompletedProcess:
    return subprocess.run(["/bin/bash", "-lc", f"set -euo pipefail; {cmd}"],
                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

def run_job(job):
    name = job["name"]
    cmd = job["runs"].strip()
    retries = int(job.get("retries", 1))
    inputs = job.get("inputs", [])
    artifacts = job.get("artifacts", [])

    cache_key = hash_inputs(cmd, inputs)
    cache_dir = Path(".dag_cache") / name
    cache_dir.mkdir(parents=True, exist_ok=True)
    marker = cache_dir / f"{cache_key}.ok"
    if marker.exists():
        print(f"âš¡ cache hit: {name}")
        return True

    attempt = 0
    while True:
        attempt += 1
        print(f"â–¶ï¸  {name} (attempt {attempt})")
        Path("artifacts").mkdir(parents=True, exist_ok=True)
        proc = run_cmd_bash(cmd)
        if proc.returncode == 0:
            marker.touch()
            collect_artifacts(name, artifacts)
            print(f"âœ… {name} succeeded")
            return True
        print(f"âŒ {name} failed (exit {proc.returncode})")
        print("â€”â€”â€” output â€”â€”â€”")
        print(proc.stdout.rstrip())
        print("â€”â€”â€”â€” end â€”â€”â€”â€”")
        if attempt > retries:
            print(f"âŒ {name} failed after {attempt} attempts")
            return False
        backoff = min(10, attempt * 2)
        print(f"ðŸ” retrying in {backoff}sâ€¦")
        time.sleep(backoff)

def run_workflow(workflow_path: str) -> bool:
    cfg = parse_yamlish(workflow_path)
    order = topo_sort(cfg["jobs"])
    ok = True
    for j in order:
        if not ok:
            print(f"â­ï¸  skipping {j['name']} (previous failure)")
            continue
        ok = run_job(j) and ok
    print("ðŸŽ‰ pipeline complete" if ok else "ðŸ’¥ pipeline failed")
    return ok

# --- Demo workflow that should pass ---
if __name__ == "__main__":
    if not Path("workflow.yml").exists():
        Path("src").mkdir(exist_ok=True)
        with open("src/app.txt", "w") as f: f.write("hello\n")
        with open("workflow.yml", "w") as f:
            f.write("""\
jobs:
  - name: build
    runs: |
      echo "building"
      mkdir -p out
      test -f src/app.txt
      cat src/app.txt > out/app.txt
      ls -l out
    retries: 1
    inputs:
      - src/**
    artifacts:
      - out/**
  - name: test
    needs:
      - build
    runs: |
      echo "testing"
      test -f artifacts/build/app.txt
      grep -q hello artifacts/build/app.txt
    retries: 1
  - name: package
    needs:
      - test
    runs: |
      echo "packaging"
      mkdir -p dist
      tar -czf dist/app.tgz -C artifacts/build app.txt
      ls -l dist
    retries: 1
    artifacts:
      - dist/**
""")
    run_workflow("workflow.yml")